\documentclass[mat1]{fmfdelo}
% \documentclass[fin1]{fmfdelo}
% \documentclass[isrm1]{fmfdelo}
% \documentclass[mat2]{fmfdelo}
% \documentclass[fin2]{fmfdelo}
% \documentclass[isrm2]{fmfdelo}

% aktivirajte pakete, ki jih potrebujete
% \usepackage{tikz}
\usepackage{mathtools} 
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

% za številske množice uporabite naslednje simbole
\newcommand{\R}{\mathbb R}
\newcommand{\N}{\mathbb N}
\newcommand{\Z}{\mathbb Z}
\newcommand{\C}{\mathbb C}
\newcommand{\Q}{\mathbb Q}

\newcommand{\F}{\mathcal F}


\newcommand{\query}{F: \R^n \to \R^d}

% matematične operatorje deklarirajte kot take, da jih bo Latex pravilno stavil
% \DeclareMathOperator{\conv}{conv}

% na razpolago so naslednja matematična okolja, ki jih kličemo s parom 
% \begin{imeokolja}[morebitni komentar v oklepaju] ... \end{imeokolja}
%
% definicija, opomba, primer, zgled, lema, trditev, izrek, posledica, dokaz
% 


% vstavite svoje definicije ...
%  \newcommand{}{}
\newcommand{\stcomp}[1]{{#1}^{\mathsf{c}}} 

% naslednje ukaze ustrezno napolnite
\avtor{Luka Lodrant} 

% TODO ali ohranimo naslov članka enak kot v izhodiščnem članku
\naslov{O geometriji diferencirane zasebnosti}
\title{On the Geometry of Differential Privacy}

% navedite ime mentorja s polnim nazivom: doc.~dr.~Ime Priimek, 
% izr.~prof.~dr.~Ime Priimek, prof.~dr.~Ime Priimek
% uporabite le tisti ukaz/ukaze, ki je/so za vas ustrezni 
\mentor{doc. dr. Aljoša Peperko}
% \mentorica{}
% \somentor{}
% \somentorica{}
% \mentorja{}{}
% \mentorici{}{}

\letnica{2018} % leto diplome

%  V povzetku na kratko opišite vsebinske rezultate dela. Sem ne sodi razlaga organizacije dela --
%  v katerem poglavju/razdelku je kaj, pač pa le opis vsebine.
\povzetek{}

%  Prevod slovenskega povzetka v angleščino. 
\abstract{}

% navedite vsaj eno klasifikacijsko oznako --
% dostopne so na www.ams.org/mathscinet/msc/msc2010.html

% TODO posvet s profesorjem o klasifikaciji
\klasifikacija{52-02}
\kljucnebesede{diferencirana zasebnost} % navedite nekaj ključnih pojmov, ki nastopajo v delu
\keywords{differential privacy} % angleški prevod ključnih besed


\begin{document}

\section{Uvod}

% Opis problema (kot v kratki predstavitvi).
% Primeri - histogram, private bits, deep learning iz on-geometry

\section{Priprava splošnega okolja}
$\mathcal{l}$
Če želimo rigurozno analizirati zasebnost podatkov moramo najprej postaviti okolje v katerem bomo lahko to počeli. \medskip

{\em Podatkovno bazo} bomo predstavili kot vektor $x \in \R^n$, {\em poizvedbo} na taki podatkovni bazi pa z linearno kombinacijo členov $x$. Natančneje lahko $d$ poizvedb združimo v linearno preslokavo $F: \R^n \to \R^d$, kjer omejimo vse koeficiente v $d \times n$ matriki na interval $[-1, 1]$. V celotnem delu bomo predpostavili tudi $d \leq n$. \medskip

Mehanizem bo v tem primeru naključen algoritem, ki kot vhod vzame podatkovno bazo $x \in \R^n$ ter poizvedbo $F: \R^n \to \R^d$, vrne pa razultat v obliki $a \in \R^d$. Tak mehanizem lahko analitik uporablja za izvajanje analiz na podatkovni bazi $x$. Neformalno bi tak mehanizem bil diferencirano zaseben, če bi se za dve dovolj podobni podatkovni bazi odgovori razlikovali za multiplikativen faktor največ $exp(\varepsilon)$. Tu je $\varepsilon$ parameter, ki pove, kako močno zaseben je obravnavani mehanizem. Manjši $\varepsilon$ pomeni višjo zasebnost. {\em Napaka} takega algoritma je pričakovana evklidska razdalja med pravilni odgovorom $F(x)$ in dejanskim odgovorom $a$. \medskip

Omenjeni naključen algoritem je tak algoritem, ki v svojem delovanju uporabi stopnjo naključnosti. Razultati z istimi vhodnimi podatki je zato načeloma različen vsakič, ko ta algoritem izvedemo. Eden najosnovnejši primerov takega algoritma je metoda Monte Carlo.

\begin{definicija}
	{\em Podatkovna baza} = vektor v Rn
\end{definicija}

\begin{definicija}
	{\em poizveda} = linearna preslikava
\end{definicija}

\section{Verjetnosta mera}

V naslednjem razdelku želimo še bolj strogo definirati pojme iz prejšnjega razdelka, a bomo za to potrebovli nekaj dodatnih teoretičnih osnov iz teorije mere. Tukaj bomo navedli le definicije uporabljenih pojmov, vse uporabljene izreke in leme pa bomo navedli sproti.

% TODO iz magajna (ali je treba definicije citirati???)
\begin{definicija}
	Naj bo $\Omega$ neprazna množica. Družino podmožic $\F$ množice $\Omega$ imenujemo {\em $\sigma$-algebra}, če ima naslednje tri lastnosti:
	\begin{enumerate}
		\item $\Omega \in \F$
		\item za vsako podmnožico $S \in \F$ je tudi $\stcomp{S} \in \F$
		\item za vsako števno družino $\{ F_i: i \in \N\}$ elementov iz $\F$ je tudi unija $\bigcup_{i \in \N} F_i$ v $\F$
	\end{enumerate}
\end{definicija}

\begin{definicija}
	Elemente družine $\F$ imenujemo {\em merljive množice}, par $(\Omega, \F)$ pa imenujemo {\em merljiv prostor}.
\end{definicija}

\begin{definicija}
	{\em Pozitivna mera} (ponavadi kar {\em mera}) na merljivem prostoru $(\Omega, \F)$ je funkcija:
	\begin{equation*}
	\mu: \F \to [0, \infty],
	\end{equation*}
	ki zadošča pogojema
	\begin{enumerate}
		\item $\mu(\emptyset)=0$
		\item $\mu(\bigcup_{n=1}^\infty F_n) = \sum_{n=1}^\infty \mu(F_n)$
	\end{enumerate}
	za vsako zaporedje disjunktnih množič $F_n \in \F$. Trojko $(\Omega, \F, \mu)$ bomo imenovali prostor z mero.
\end{definicija}

\begin{definicija}
	Meri $\mu$ pravimo {\em verjetnostna mera}, če velja $\mu(\Omega) = 1$.
\end{definicija}

\section{Diferencirana zasebnost}

\begin{definicija}
	{\em Mehanizem} M je družina verjetnostnih mer $M = \{\mu_x: x \in \R^n \}$, kjer je vsak $\mu_x$ definiran na $\R^d$.
\end{definicija}

\begin{definicija}
	Mehanizem je {\em $\varepsilon$-diferencirano zaseben}, če za vse $x,y \in \R^n$ za katere je $\norm{x-y}_1 \leq 1$ velja $sup_{S \subseteq \R^d} \frac{\mu_x(S)}{\mu_y(S)} \leq exp(\varepsilon)$, kjer supremum teče čez vse merljive podmnožice $S \subseteq \R^n$.
\end{definicija}

Pogosta je tudi šibkejša oblika $\varepsilon$-diferencirane zasebnosti.

\begin{definicija}
	Mehanizem je {\em $\delta$-približno $\varepsilon$-diferencirano zaseben}, če za vse $x,y \in \R^n$ za katere je $\norm{x-y}_1 \leq 1$ velja $\mu_x(U ) \leq exp(\varepsilon) \mu_y(S) + \delta$.
\end{definicija}

Za obravnavo diferencirano zasebnih mehanizmov sta pomembna tudi pojma napake in občutljivosti.

\begin{definicija}
	{\em Napako} mehanizmo $M$ po normi $\ell$ befiniramo kot $err_\ell(M, F)=\sup_{x \in \R^n} \mathbb{E}_{a \sim \mu_x }(\norm{a-Fx}_\ell)$. Tu je kot prej $F: \R^n \to \R^d$.
\end{definicija}

\begin{definicija}
	Mehanizem je {\em $\delta$-približno $\varepsilon$-diferencirano zaseben}, če za vse $x,y \in \R^n$ za katere je $\norm{x-y}_1 \leq 1$ velja $\mu_x(U ) \leq exp(\varepsilon) \mu_y(S) + \delta$.
\end{definicija}


\section{Spodnja meja prek ocene volumna}

\begin{definicija}
	Množica točk $Y \subseteq \R^d$ se imenuje {\em $r$-pakiranje }, če je $\norm{y-y'}_2 \geq r$ za vsak $y, y' \in Y, y \neq y'$.
\end{definicija}

\begin{trditev}
	Naj bo $K \subseteq \R^d$ in $R=Vol(K)^{1/d}$. Potem $K$ vsabuje $\Omega(R\sqrt{d})$-pakiranje velikosti vsak $\exp(d)$.
\end{trditev}

\begin{izrek}
	Naj bo $\varepsilon > 0$, $\query$ linearna preslikav in $K=FB_1^n$. Potem ima vsak $\varepsilon$-zaseben mehanizem $M$ napako vsaj $err(M,F) \geq \Omega(\varepsilon^-1 d \sqrt(d) \cdot Vol(k)^{1/d}$.
\end{izrek}

\begin{dokaz}
	Naj bo $\lambda \geq 1$ in $R=Vol(K)^{1/d}$. 	
\end{dokaz}

\section{K-norma mehanizem}

To je 

% Mislim da bo dovolj če pridemo le do sem v izhodiščnem članku, pod pogojem, da bo vse dobro razloženo. Po potrebi še

%\section{Matching bounds}

\section*{Slovar strokovnih izrazov}

%TODO predebatirati z mentorjem o prevodih, ni še slo literature
\geslo{diferencirana zasebnost}{differential privacy}
\geslo{približna diferencirana zasebnost}{approximate differential privacy}
\geslo{podatkovna baza}{database}
\geslo{poizvedba}{query}
\geslo{naključen algoritem}{random algorithm}

% seznam uporabljene literature
\begin{thebibliography}{99}

\bibitem{on-geometry}
M.~Hardt in K.~Talwar, \emph{On the Geometry of Differential Privacy}, 9.11.~2009, dostopno na \url{https://arxiv.org/abs/0907.3754}.

\bibitem{deep-learning}
M.~Abadi, A.~Chu, I.~Goodfellow, H.~B.~McMahan, I.~Mironov, K.~Talwar, L.~Zhang, \emph{Deep Learning with Differential Privacy}, 25.~10.~2016, dostopno na \url{https://arxiv.org/abs/1607.00133}.

\bibitem{teorija-mere}
B.~Magajna, \emph{Osnove teorije mere}, Podiplomski seminar iz matematike 27, DMFA -- založništvo, Ljubljana, 2011


\end{thebibliography}

\end{document}

